{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A long short-term memory recurrent neural network for generating astrophysics-specific language and assessment using tf-idf, cosine similarity, and presence of non-ASCII characters to determine model effectiveness\n",
    "#### Daina Bouquin\n",
    "    \n",
    "This is an expanded version of [the initial proposal](https://github.com/dbouquin/DATA_698/blob/master/DATA698_proposal.Rmd) that includes discussion of prior approaches, baseline results of my approach, and a final assessment of whether this project can effectively address the goals stated in the initial proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction   \n",
    "In 1987, a very simple definition of an \"artificial neural network\" (ANN) was presented by Maureen Caudill, an expert on artificial intellegence. Caudill wrote that an artifical neural network is \"...a computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs\" [1]. With this definition in mind, what I propose herein is a small study on the application of a particular type of recurant ANN, long short-term memory (LSTM), to generate domain-specific text in various subdisciplines of astronomy and astrophysics (research on black holes, astrobiology, and exoplanets). Subsequent to the training and sampling of astronomy and astrophysics related text, I will apply measures including term frequency-inverse document frequency (tf-idf) and cosine similarity, along with a vector space model to assess the degree to which the text generated by the ANN accurately reflects the text used to train the system. I will also use the proportion of non-ASCII characters generated by the LSTM models as an indicator of the success of the model. \n",
    "\n",
    "In conducting this study I will be applying the aforementioned approaches to a dataset that has not yet been used to test the capacity for this type of system to \"learn\": metadata from the SAO/NASA Astrophysics Data System. I will also be applying an LSTM ANN to in a jargon-heavy scientific domain, which is an area of limited research. Moreover, this analysis stands to help foster conversations within the library science community about the challenges non-scientists face in improving their scientific literacy and engaging with academic publications. Specifically, the ANN-generated \"fake\" astronomy text could potentially be used as control text in future literacy studies and the degree of success achieved in emulating the text could serve as an indicator of the complexity inherent to this kind of discipline-specific language. Moreover, the domains within astronomy and astrophysics that I choose to compare to one another will reepresent fields with differing longevity and I will therefore be able to make inferences as to how the age of the field may impact the model's ability to emulate the text. Furthermore, sharing the ANN-generated \"fake\" astronomy text with the astronomy community and others will allow us to further frame conversations about \"fake news\" and scientific literacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "The SAO/NASA Astrophysics Data System (ADS) [2], is an online database of over eight million astronomy and physics papers from both peer reviewed and non-peer reviewed sources. The ADS is a highly used resource in the astronomy and astrophysics communities and has many levels of indexing. The ADS API makes it possible to query this valuable resource to better understand authorship and publishing behavior in these fields among many other applications. The ADS allows for full-text searching, and in the near future, will be fully incorporating [Unified Astronomy Thesaurus](http://astrothesaurus.org/) keywords into their indexing schema. The ADS is managed by the Smithsonian Astrophysical Observatory at the Harvard–Smithsonian Center for Astrophysics with funding from NASA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background   \n",
    "Artificial neural networks can be understood as being organized into \"layers\" with layers being made up of a number of interconnected \"nodes\" which each contain an \"activation function\". Data are passed into the \"input layer\" of the system, which communicates to \"hidden layers\" where the actual processing is done via a system of weighted \"connections\". Subsequently, the hidden layers connect with an \"output layer\" where the answer is output [3]. Since the creation of ANNs, these layered connections of nodes have been implementated in many different ways. Each implementation has been designed to meet different needs and experiments. Perhaps the most recognizable application of ANNs in popular culture is Google's \"Deep Dream\", a computer vision program that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia [4]. \n",
    "\n",
    "In text-based applications of ANNs though, recurant neural networks have become favored over convolutional systems. For instance, Twitter bots like \"Deep Drumpf\" [5] have been used to demonstrate the powerful effectiveness of recurrant ANNs by emulating the linguistical style and syntax of individual people. Similarly, people have used recurrant ANNs to generate text based on the plays of Shakespeare [6], the essays of Paul Graham [7] and even to generate baby names [8]. Andrej Karpathy, Tesla Motors' leader on artifical intelligence reserach, has stated that some of the reasoning behind the \"unreasonable effectiveness of recurrant neural networks\" is that with convolutional systems, \"their API is too constrained\" in that they accept only a fixed-sized vector as input and subsequently produce a fixed-sized vector as output using a fixed amount of computational steps. Unlike these convolutional ANNs, recurant ANNs allow us to operate over sequences of vectors: sequences in the input, output, or even both [9].\n",
    "\n",
    "To further define and justify the approach to be used for the herein described experiment, it is necessary to define the specific recurant ANN I will be employing: long short-term memory (LTSM). LTSM ANNs are \"capable of learning long-term dependencies\" [10] and do not start from scratch whenever they are presented with new data. LTSMs were introduced by Hochreiter and Schmidhuber in 1997 [11] and were refined iteratively following their work. The LTSM approach to recurant ANNs is ideal for the type of text-based analysis I will be conducting because by using LTSM you can train the system on a large corpus of text and it will \"learn\" to generate text like it one character at a time. This, along with LTSM's relative insensitivity to \"gap length\" gives an advantage to LSTM over alternative recurant neural networks [12]. Note, I have not found any evidence so far showing this approach being applied to text from astrophysics articles or using language from such a scientific-jargon heavy field.\n",
    "\n",
    "<img src=\"greff_lstm_diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cases\n",
    "In order to test out the capability of a LSTM recurant ANN to work effectively with astronomy-related text, I will subset increasingly large datasets from the metadata available through the Astrophysics Data System. I will use the titles to train the herein described ANN and to generate test titles. I will do this for different subdisciplines within the field. I will gather baseline metrics for each training dataset that I will be able to use to compare differences between \"real\" training data and \"fake\" data generated from the LSTM models. The metrics I will gather are as follows:\n",
    "* Total number of unique words in various sizes of training sets within a domain\n",
    "* Average proportion of non-ASCII characters in various sizes of training sets within a domain\n",
    "\n",
    "Non-ASCII characters are characters that do not fall within the American Standard Code for Information Interchange ([ASCII](https://en.wikipedia.org/wiki/ASCII) [13]) character encoding scheme.\n",
    "\n",
    "#### Data Sets\n",
    "I have chosen to extract from the SAO/NASA Astrophysics Data System metadata from three different domains within astronomy and astrophysics. The datasets I have built to train the LSTM artificial neural network are available for download [here](https://github.com/dbouquin/DATA_698/tree/master/Training_Datasets) and are made up of increasingly large lists of titles from three specific domains. The lists range in length from the most recently published 1,000 titles of papers written in their prospective fields, up to the most recent 35,000 papers indexed by ADS. It is important to note here though that due to limitations in processing capability I have only yet performed baseline analyses for training datasets up to 20,000 titles and may not reach a capacity to process 35,000 titles.\n",
    "The three domains within astronomy and astrophysics that I chose to analyse are:\n",
    "* Black Holes - [earliest publication available in ADS is 1799](https://ui.adsabs.harvard.edu/#abs/1799AllGE...4....1L/abstract)\n",
    "* Astrobiology - [earliest publication available in ADS is 1870](https://ui.adsabs.harvard.edu/#abs/1870Natur...3...86M/abstract)\n",
    "* Exoplanets - [earliest publication available in ADS is 1943](https://ui.adsabs.harvard.edu/#abs/1943MNSSA...2...14V/abstract)\n",
    "\n",
    "These domains were chosen in order to enable the comparison of increasingly recent subdisciplines within astronomy and astrophysics and to ensure that there is substantial data to pull from ADS.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "My hypthothesis is that some fields will be more acurately generated by the LSTM ANN than others. The relationship will be correlated with the diversity of unique words used within the training corpus of text representing the subdomain. This is to say that a subdomain with a larger variety of unique words (combinations of letters) will be more challenging to for the LSTM ANN to generate than a training corpus with fewer unique words. I hypothesize that more data will be needed to achieve a high degree of similarity between generated text and training text in some fields than others and that this will similarly correspond to unique word count. I also hypothesize that the ability to generate a similar title to those available to train the model in the original training sets will be different between fields that are more emergent compared with those that are more established. For instance, I hypothesize that the LSTM ANN will be more effective in generating titles based on the \"black hole\" data than on the \"exoplanet\" data as the exoplanet field is much less established and therefore likely less regular from a linguistics perspective than text discussing black holes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch for Data Analysis\n",
    "[why]\n",
    "[pros/cons]\n",
    "#### Installing Torch\n",
    "Working with Torch means working with a system that has multiple dependencies including Python and Lua, and if you have access to hardware like an NVidia graphics card, CUDA and OpenCL for GPU accelleration are also helpful. Standard instructions for installing Torch on various operating systems are available [here](https://github.com/jcjohnson/torch-rnn) [20]. I do not have access to GPU resources locally and followed Torch's standard [OSX installation instructions](http://www.jeffreythompson.org/blog/2016/03/25/torch-rnn-mac-install/) [21]. Unfortunately this means that the amount of time that it takes to train my LSTM models is extensive and serves as a limitation to this study. With more time and resources it would be possible to expand this analysis into the cloud to speed the compuation along, however, it is still possible to run the herein described tests without cloud computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis Full Workflow Demonstration\n",
    "The below code cells are used to demonstrate the creation of the initial datasets for training the LSTM models and include all necessary preprocessing steps. They also show how baseline metrics about the training sets can be calculated. The creation of file formats needed for use in Torch as also shown as screenshots (running Torch in the notebook is impractical and will vary between local operating systems). To reproduce the torch analysis below without needing to install Torch or create a separate virtual environment as described above, it is possible to make use of [Torch's various docker images](https://github.com/crisbal/docker-torch-rnn) [15]. The final code cells are used to show the training of the model and subsequent sampling of data resulting from the training. I then demonstrate how cosine similarity can be calculated between a randomly sampled \"fake\" title generated by the LSTM model and the training corpus itself.     \n",
    "This demonstration is meant to establish the feasibility of the study and show that these methods can be used to explore the hypotheses outlined herein. The work of the remainer of the project will be to compare these analyses across disciplines and training dataset sizes. Visualizations will be created as part of the final draft of this project to illustrate whether or not my hypotheses can be confirmed as predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Extraction and Preprocessing\n",
    "The below scripts represent and example of how data is extracted from the NASA Astrophysics Data System API and converted into a text files for subsequent processing and analysis using [torch](http://torch.ch/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pulling Data from the Astrophysics Data System \n",
    "Example for 1,000 most recent \"Black Hole\" titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Character Length per Training Title: 85\n"
     ]
    }
   ],
   "source": [
    "# modules for ADS API and data transformations\n",
    "import os\n",
    "import ads as ads \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# configure API\n",
    "os.environ[\"ADS_DEV_KEY\"] = \"kNUoTurJ5TXV9hsw9KQN1k8wH4U0D7Oy0CJoOvyw\"\n",
    "ads.config.token = 'ADS_DEV_KEY'\n",
    "\n",
    "papers1 = list(ads.SearchQuery(q= \"black hole\", fl=['bibcode', 'title', 'abstract'], sort='pubdate', max_pages=20 ))\n",
    "\n",
    "# find titles \n",
    "t = []\n",
    "for i in papers1:\n",
    "    title1 = i.title\n",
    "    t.append(title1)\n",
    "title = t\n",
    "\n",
    "# create an initial df (only 1 column) and clean it up\n",
    "df = pd.DataFrame({'Title' : title\n",
    "  })\n",
    "df['Title'] = df['Title'].str.get(0)\n",
    "\n",
    "# write to .txt\n",
    "df.to_csv(\"blackhole_1000.txt\", sep=' ', header=None, index=None, encoding='utf-8')\n",
    "\n",
    "# get average number of characters in a title for use in torch\n",
    "avg_char_len = sum(df['Title'].str.len())/1000\n",
    "\n",
    "print \"Average Character Length per Training Title:\", avg_char_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above calculation will be used to specify the length of the title that Torch will generate. In this case we will ask Torch to generate \"fake\" titles that are 85 characters long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Average Proportions of Non-ASCII Characters in Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average count of non-ASCII characters per training title with non-ASCIIs: 3\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Remove unnecessary white space and read data\n",
    "lines1K = [line.rstrip('\\n') for line in open('blackhole_1000.txt')]\n",
    "\n",
    "# Find titles with non-ascii characters\n",
    "nonascii_lines1K = []\n",
    "for line in lines1K:\n",
    "  a = re.sub('[\\x00-\\x7f]', '', line)\n",
    "  nonascii_lines1K.append(a)\n",
    "\n",
    "# Drop empty elements in the list (titles with no ascii characters)\n",
    "nonascii_notempty1K = filter(None, nonascii_lines1K)\n",
    "count_nonascii_notempty1K = len(nonascii_notempty1K)\n",
    "\n",
    "# Calculate average number of non-ascii characters in a title when the title contains non-ascii characters\n",
    "actual_nonascii_1K = sum([len(i) for i in nonascii_notempty1K])/count_nonascii_notempty1K\n",
    "\n",
    "print \"Average count of non-ASCII characters per training title with non-ASCIIs:\", actual_nonascii_1K "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, the non-ASCII analysis shows that when titles in our training set contain characters that are not ASCII-encoded, there are on average three non-ASCII characters in that title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Total Number of Unique Words in Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique words: 4134\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "\n",
    "with open('blackhole_1000.txt', 'r') as myfile:\n",
    "    blackhole_1000=myfile.read().replace('\\n', '')\n",
    "    \n",
    "print \"Number of Unique words:\", len(Counter(blackhole_1000.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have hypothesized that having more unique words in a training set will impact the ability for the model to accurately generate \"fake\" titles. This is because the LSTM recurrant ANN will generate the titles one character at a time, and I hypothesize that more unique words will impact the system's ability to predict upcoming characters.   \n",
    "    \n",
    "Now that we have built our training dataset made up of 1,000 \"black hole\" titles from the ADS and calculated the average length of a title, number of unique words in the training set, and average number of non-ASCII titles in a title when non-ASCII characters are present, we can begin processing the training dataset for use with Torch.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building JSON and H5 files for Torch\n",
    "Torch requires an analyst to create two files in order to take advantage of Torch's library of machine learning code for LSTM ANN models: a JSON file representing the text in a more standard structure, and a [hierarchical data format](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) [14] called an H5 file, which is designed to organize and store very large amounts of data.   \n",
    "    \n",
    "To create the two necessary files for training the LSTM ANN, Torch makes available preprocessing Python scripts can can easily be run in the terminal that are available [here](https://github.com/jcjohnson/torch-rnn/blob/master/scripts/preprocess.py) [22]. It is important to make sure that when running the preprocessing scripts that your file directory structure matches the designated structure (i.e. run the preprocessing code while in the torch-rnn directory: `~/torch/torch-rnn`). The below screenshot shows how I can run the preprocessing scripts and generate the needed JSON and H5 files. The output from the preprocessing script also prints metrics representing the training data.    \n",
    "    \n",
    "*Note, this screenshot results from applying the preprocessing scripts to the [final \"blackhole_1000.txt\" dataset](https://github.com/dbouquin/DATA_698/blob/master/Final_Datasets/blackholes/blackhole_1000/blackhole_1000.txt) [23], rather than from the newly run code demonstration written here. The code written here pulls the most recent 1,000 papers on black holes from ADS, which will differ from my original analysis.*\n",
    "<img src=\"img/preprocessing_blackhole1000.png\"style=\"width: 850px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the LSTM Model Using Torch\n",
    "<img src=\"img/training_blackhole1000.png\"style=\"width: 850px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Test Sample using the LSTM Model Generated by Torch\n",
    "The script run in the screenshot below does two things: it generates a single example title, and it also builds a file made up of 100 example titles generated by the model. You can see 1,000 example titles generated by Torch using the 1,000 title training set of black hole titles [here](https://github.com/dbouquin/DATA_698/blob/master/Final_Datasets/blackholes/blackhole_1000/blackhole_1000_sample.txt) [24].\n",
    "<img src=\"img/sampling_blackhole1000.png\"style=\"width: 850px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment\n",
    "The following code cells demonstrate how cosine similarity calculated on tf-idf vectors will be used to assess the effectiveness of the LSTM model. Similarly, I will also demonstrate an assessment involving the proportion of non-ASCII characters in training and sampled datasets.\n",
    "    \n",
    "#### Cosine Similarity\n",
    "Now that we have processed our data, trained our model, and generated sample titles, we can apply various measures to assess how the model faired in generating titles that are representative of the training titles themselves. To do this we will take advantage of tools that have become popular and impactful in the field of digital libraries. First we will need to use the measure \"term frequency-inverse document frequency\" (tf-idf), which is a numerical statistic that is intended to reflect how important a word is to a document in a corpus of text [13]. Tf-idf is a measure that is already employed by the ADS in topic modeling and visualizations available through the ADS \"explore\" features in their \"Bumblebee\" interface. This value will allow me to take advantage of an both cosine similarity and a vector space model to determine how similar two titles actually are to each other.    \n",
    "Below I will show how I can compare a random title that was generated by my LSTM network to the corpus of real titles on which the model was trained. I did this with four different datasets of increasing size for illustration [here](https://github.com/dbouquin/DATA_698/blob/master/analysis/cosine_sim_analysis/Cosine_similarity.ipynb) [25]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM ANN Generated Title:\n",
      "αheodiation in black hole atoveriom durings in rateriations in variability in rela  \"Dynamic collapses of relativistic degenerate stellar cores and radiation pressure dominated stellar interiors\"\n",
      "--------------------------------------------\n",
      "Most Similar Real Title:\n",
      "\"Stellar populations, stellar masses and the formation of galaxy bulges and discs at $z < 3$ in CANDELS\"\n",
      "--------------------------------------------\n",
      "Cosine Similarity Angle: 78.4887491248\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# import modules needed for tf-idf and cosine similarity calculations \n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer() #term frequency inverse document frequency\n",
    "\n",
    "# import test file\n",
    "# ** the first line of the test file is randomly selected title \n",
    "# generated by LSTM model that was trained on 1000 black hole titles **\n",
    "blackhole_test1K = [line.rstrip('\\n') for line in open('blackhole_1000_test.txt')]\n",
    "df_blackhole_test1K = pd.DataFrame(blackhole_test1K)\n",
    "df_blackhole_test1K.columns = ['Title']\n",
    "\n",
    "# convert title column to tuple\n",
    "title_tuple = tuple(list(df_blackhole_test1K['Title']))\n",
    "\n",
    "# create TFIDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(title_tuple)\n",
    "\n",
    "# calculate the cosine similarity of sample title compared to the whole training set\n",
    "# Compare the first title (the test title) to all other titles\n",
    "cosine_sim_array = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "\n",
    "# Show most similar title in degrees\n",
    "cos_sim = np.partition(cosine_sim_array.flatten(), -2)[-2] # second largest value in array (most similar title)\n",
    "angle_in_degrees = math.acos(cos_sim)\n",
    "most_similar1K = math.degrees(angle_in_degrees)\n",
    "\n",
    "# find index of most similar title\n",
    "itemindex = np.where(cosine_sim_array==cos_sim)\n",
    "itemindex = itemindex[1]\n",
    "itemindex = itemindex[0]\n",
    "\n",
    "# find most similar title and print it along with first title used for comparison\n",
    "comparison_title = title_tuple[0]\n",
    "most_similar_title = title_tuple[itemindex]\n",
    "\n",
    "print \"LSTM ANN Generated Title:\"\n",
    "print comparison_title\n",
    "print \"--------------------------------------------\"\n",
    "print \"Most Similar Real Title:\"\n",
    "print most_similar_title\n",
    "print \"--------------------------------------------\"\n",
    "print \"Cosine Similarity Angle:\", most_similar1K\n",
    "print \"--------------------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity Explained\n",
    "Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them [14] and is commonly applied in settings where tf-idf vectors are being compared. Cosine similarity can be calculated thusly:\n",
    "<img src=\"cos_sim_calc.png\"style=\"width: 400px;\">\n",
    "    \n",
    "As illustrated below, the similarity between two vectors (strings of characters) increases as the angle between the two vectors decreases. When the angle is 90 degrees, the two vectors are unrelated.    \n",
    "<img src=\"cosinesimilarity.png\"style=\"width: 700px;\">\n",
    "    \n",
    "The angle calculated between the test case above comparing a randomly selected title generated using the LSTM model trained on 1,000 real black hole titles to the corpus of training titles itself shows that the sampled title is measurably similar to the training data. This proof of concept will act as the foundation on which I will scale my study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Non-ASCII Characters\n",
    "The below code analyses the results of the training and sampling of text from the LSTM ANN from the perspective of the presence or absence of non-ASCII characters. For this analysis I will make use of 100 LSTM generated titles and compare the proportion of non-ASCII characters per title containing non-ASCII characters to that of the training corpus itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average count of non-ASCII characters per LSTM generated title with non-ASCIIs: 37\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary white space and read data\n",
    "test_lines100 = [line.rstrip('\\n') for line in open('blackhole_1000_sample.txt')]\n",
    "\n",
    "# Find titles with non-ascii characters\n",
    "nonascii_test_lines100 = []\n",
    "for line in test_lines100:\n",
    "  a = re.sub('[\\x00-\\x7f]', '', line)\n",
    "  nonascii_test_lines100.append(a)\n",
    "\n",
    "# Drop empty elements in the list (titles with no ascii characters)\n",
    "nonascii_notempty_test100 = filter(None, nonascii_test_lines100)\n",
    "count_nonascii_notempty_test100 = len(nonascii_notempty_test100)\n",
    "\n",
    "# Calculate average number of non-ascii characters in a title when the title contains non-ascii characters\n",
    "actual_nonascii_100 = sum([len(i) for i in nonascii_notempty_test100])/count_nonascii_notempty_test100\n",
    "\n",
    "print \"Average count of non-ASCII characters per LSTM generated title with non-ASCIIs:\", actual_nonascii_100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above analysis, when an LSTM generated title contains non-ASCII characters, it contains far more non-ASCII characters than the training titles do. This analysis was done using sample text from an LSTM model trained on 1,000 real black hole titles. To supplement this analysis I have demonstrated [here](https://github.com/dbouquin/DATA_698/blob/master/analysis/ascii_analysis/ascii_exoplanets_vis.ipynb) [26] how the proportion of non-ASCII characters gets increasingly realistic as the training size of the data increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "The process that I have defined here of sampling data from the Astrophysics Data System, processing it for use in training an LSTM ANN model using Torch, generating sample titles from the trained model, and subsequently analyzing the results, can be used to test the hypothesis I have layed out for this project. The limitations of this approach are not inherent to the process, but instead are linked to the capabilities of the hardware I am employing to train the neural network and the bandwidth throttling of the Astrophysics Data System API, which times out at attempts to pull datasets greater than 35,000 article records. My personal computer has proved incapable of running processes to train models using datasets greater than 20,000 titles. LSTM neural networks are inherently slow to train despite their effectiveness.\n",
    "\n",
    "### Next Steps\n",
    "The next steps for this project are to execute the above workflow at a larger scale. I will compare the above results to the results of the same analyses using training sets from three different domains within astronomy with increasingly large datasets to train my models (1K, 5K, 10K, and 20K titles). I will then be able to make generalizations about how the LSTM does in emulating text from three fields and infer ideas about how the fields differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "The above described analysis can be effective in examining the complexity inherent in different subfields of astronomy and astrophysics. Once scaled to include analyses of text generated using larger training datasets from other subfields, I will be able to test my hypotheses by making analytical and visual comparisons between the fields in question. The LSTM ANN approach does work as desired, however, training time is slow and would be more practical with additional computing resources. Future analysis using cloud infrastructure may be explored as a addendum to this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] https://dl.acm.org/citation.cfm?id=38295   \n",
    "[2] https://ui.adsabs.harvard.edu/    \n",
    "[3] http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html      \n",
    "[4] https://arxiv.org/abs/1409.4842    \n",
    "[5] http://www.deepdrumpf2016.com/about.html    \n",
    "[6] https://github.com/RuthAngus/Fakespeare    \n",
    "[7] https://suriyadeepan.github.io/2017-02-13-unfolding-rnn-2/    \n",
    "[8] https://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/     \n",
    "[9] https://karpathy.github.io/2015/05/21/rnn-effectiveness/    \n",
    "[10] https://en.wikipedia.org/wiki/Long_short-term_memory    \n",
    "[11] http://www.bioinf.jku.at/publications/older/2604.pdf    \n",
    "[12] https://colah.github.io/posts/2015-08-Understanding-LSTMs/  \n",
    "[13] https://en.wikipedia.org/wiki/ASCII   \n",
    "[13] http://infolab.stanford.edu/~ullman/mmds/book.pdf     \n",
    "[14] https://en.wikipedia.org/wiki/Hierarchical_Data_Format    \n",
    "[14] https://en.wikipedia.org/wiki/Cosine_similarity       \n",
    "[15] https://github.com/crisbal/docker-torch-rnn    \n",
    "[16] http://www.deeplearning.net/software/theano/    \n",
    "[17] http://torch.ch/    \n",
    "[18] https://www.tensorflow.org/         \n",
    "[19] https://gist.github.com/karpathy/d4dee566867f8291f086  \n",
    "[20] https://github.com/jcjohnson/torch-rnn    \n",
    "[21] http://www.jeffreythompson.org/blog/2016/03/25/torch-rnn-mac-install/    \n",
    "[22] https://github.com/jcjohnson/torch-rnn/blob/master/scripts/preprocess.py   \n",
    "[23] https://github.com/dbouquin/DATA_698/blob/master/Final_Datasets/blackholes/blackhole_1000/blackhole_1000.txt    \n",
    "[24] https://github.com/dbouquin/DATA_698/blob/master/Final_Datasets/blackholes/blackhole_1000/blackhole_1000_sample.txt    \n",
    "[25] https://github.com/dbouquin/DATA_698/blob/master/analysis/cosine_sim_analysis/Cosine_similarity.ipynb   \n",
    "[26] https://github.com/dbouquin/DATA_698/blob/master/analysis/ascii_analysis/ascii_exoplanets_vis.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
